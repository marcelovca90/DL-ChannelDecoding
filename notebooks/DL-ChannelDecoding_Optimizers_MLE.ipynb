{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Deep Learning-Based Channel Decoding\n",
    "\n",
    "If you want to cite this notebook, please use the following bibtext entry:\n",
    "\n",
    "    @article{nn-decoding,\n",
    "      title={On Deep Learning-Based Channel Decoding},\n",
    "      author={Tobias Gruber and\n",
    "              Sebastian Cammerer and\n",
    "              Jakob Hoydis and\n",
    "              Stephan ten Brink}\n",
    "      journal={CoRR}\n",
    "      year={2017}\n",
    "      url= {http://arxiv.org/abs/1701.07738}\n",
    "    }\n",
    "\n",
    "Running this example requires Keras installed with the Theano backend. For GPU support nvidia-docker is required. A Dockerfile is provided to employ this setup quickly.\n",
    "\n",
    "Our simulation setup was inspired by material from http://radioml.org.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.models import Sequential\n",
    "import base64\n",
    "import json\n",
    "import keras as k_tmp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf_tmp\n",
    "import timeit\n",
    "%matplotlib inline\n",
    "print(K.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marce\\AppData\\Local\\Temp/ipykernel_14800/2067036094.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4440054536635608889\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6133857516087789844\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3160663655\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13165885313889516480\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 960, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2457684618645204125\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available(cuda_only=True))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(str(device_lib.list_local_devices()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8                       # number of information bits\n",
    "N = 16                      # code length\n",
    "train_SNR_Eb = 1            # training-Eb/No\n",
    "\n",
    "nb_epochs = [2**x for x in [10]] # number of learning epochs\n",
    "codes = ['polar','random']                   # type of code ('random' or 'polar')\n",
    "design = [128, 64, 32]                       # each list entry defines the number of nodes in a layer\n",
    "batch_size = 256                             # size of batches for calculation the gradient\n",
    "LLR = False                                  # 'True' enables the log-likelihood-ratio layer\n",
    "loss = 'mse'                                 # or 'binary_crossentropy'\n",
    "\n",
    "# available optimizers can be found at https://keras.io/api/optimizers/\n",
    "optimizers = ['sgd','rmsprop','adam','adadelta','adagrad','adamax','nadam']\n",
    "\n",
    "train_SNR_Es = train_SNR_Eb + 10*np.log10(k/N)\n",
    "train_sigma = np.sqrt(1/(2*10**(train_SNR_Es/10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretic_symbol_error_rate(Es_No_dB):\n",
    "    Es_No = 10.0**(Es_No_dB/10.0)\n",
    "    return 0.5*math.erfc(np.sqrt(Es_No))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulateBPSK(x):\n",
    "    return -2*x +1;\n",
    "\n",
    "def addNoise(x, sigma):\n",
    "    w = K.random_normal(K.shape(x), mean=0.0, stddev=sigma)\n",
    "    return x + w\n",
    "\n",
    "def ber(y_true, y_pred):\n",
    "    return K.mean(K.not_equal(y_true, K.round(y_pred)))\n",
    "\n",
    "def return_output_shape(input_shape):  \n",
    "    return input_shape\n",
    "\n",
    "def compose_model(layers):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    return model\n",
    "\n",
    "def log_likelihood_ratio(x, sigma):\n",
    "    return 2*x/np.float32(sigma**2)\n",
    "\n",
    "# https://pt.coredump.biz/questions/45328314/kerastensorflow-adding-a-new-metric-with-an-inequality\n",
    "def errors(y_true, y_pred):\n",
    "    return K.sum(K.cast(K.not_equal(y_true, K.round(y_pred)),'float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_adder(a,b):\n",
    "    s = a ^ b\n",
    "    c = a & b\n",
    "    return s,c\n",
    "\n",
    "def full_adder(a,b,c):\n",
    "    s = (a ^ b) ^ c\n",
    "    c = (a & b) | (c & (a ^ b))\n",
    "    return s,c\n",
    "\n",
    "def add_bool(a,b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('arrays with different length')\n",
    "    k = len(a)\n",
    "    s = np.zeros(k,dtype=bool)\n",
    "    c = False\n",
    "    for i in reversed(range(0,k)):\n",
    "        s[i], c = full_adder(a[i],b[i],c)    \n",
    "    if c:\n",
    "        warnings.warn(\"Addition overflow!\")\n",
    "    return s\n",
    "\n",
    "def inc_bool(a):\n",
    "    k = len(a)\n",
    "    increment = np.hstack((np.zeros(k-1,dtype=bool), np.ones(1,dtype=bool)))\n",
    "    a = add_bool(a,increment)\n",
    "    return a\n",
    "\n",
    "def bitrevorder(x):\n",
    "    m = np.amax(x)\n",
    "    n = np.ceil(np.log2(m)).astype(int)\n",
    "    for i in range(0,len(x)):\n",
    "        x[i] = int('{:0{n}b}'.format(x[i],n=n)[::-1],2)  \n",
    "    return x\n",
    "\n",
    "def int2bin(x,N):\n",
    "    if isinstance(x, list) or isinstance(x, np.ndarray):\n",
    "        binary = np.zeros((len(x),N),dtype='bool')\n",
    "        for i in range(0,len(x)):\n",
    "            binary[i] = np.array([int(j) for j in bin(x[i])[2:].zfill(N)])\n",
    "    else:\n",
    "        binary = np.array([int(j) for j in bin(x)[2:].zfill(N)],dtype=bool)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def bin2int(b):\n",
    "    if isinstance(b[0], list):\n",
    "        integer = np.zeros((len(b),),dtype=int)\n",
    "        for i in range(0,len(b)):\n",
    "            out = 0\n",
    "            for bit in b[i]:\n",
    "                out = (out << 1) | bit\n",
    "            integer[i] = out\n",
    "    elif isinstance(b, np.ndarray):\n",
    "        if len(b.shape) == 1:\n",
    "            out = 0\n",
    "            for bit in b:\n",
    "                out = (out << 1) | bit\n",
    "            integer = out     \n",
    "        else:\n",
    "            integer = np.zeros((b.shape[0],),dtype=int)\n",
    "            for i in range(0,b.shape[0]):\n",
    "                out = 0\n",
    "                for bit in b[i]:\n",
    "                    out = (out << 1) | bit\n",
    "                integer[i] = out\n",
    "        \n",
    "    return integer\n",
    "\n",
    "def polar_design_awgn(N, k, design_snr_dB):  \n",
    "        \n",
    "    S = 10**(design_snr_dB/10)\n",
    "    z0 = np.zeros(N)\n",
    "\n",
    "    z0[0] = np.exp(-S)\n",
    "    for j in range(1,int(np.log2(N))+1):\n",
    "        u = 2**j\n",
    "        for t in range(0,int(u/2)):\n",
    "            T = z0[t]\n",
    "            z0[t] = 2*T - T**2     # upper channel\n",
    "            z0[int(u/2)+t] = T**2  # lower channel\n",
    "        \n",
    "    # sort into increasing order\n",
    "    idx = np.argsort(z0)\n",
    "        \n",
    "    # select k best channels\n",
    "    idx = np.sort(bitrevorder(idx[0:k]))\n",
    "    \n",
    "    A = np.zeros(N, dtype=bool)\n",
    "    A[idx] = True\n",
    "        \n",
    "    return A\n",
    "\n",
    "def polar_transform_iter(u):\n",
    "\n",
    "    N = len(u)\n",
    "    n = 1\n",
    "    x = np.copy(u)\n",
    "    stages = np.log2(N).astype(int)\n",
    "    for s in range(0,stages):\n",
    "        i = 0\n",
    "        while i < N:\n",
    "            for j in range(0,n):\n",
    "                idx = i+j\n",
    "                x[idx] = x[idx] ^ x[idx+n]\n",
    "            i=i+2*n\n",
    "        n=2*n\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare, Train and Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "modulator (Lambda)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "noise (Lambda)               (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 12,776\n",
      "Trainable params: 12,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2021-12-30 00:08:49.153947\tpolar @ sgd @ Mep=2^10 fit started.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14800/3858524082.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{datetime.now()}\\t{code} @ {optimizer} @ Mep=2^{nb_epoch_exp} fit started.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{datetime.now()}\\t{code} @ {optimizer} @ Mep=2^{nb_epoch_exp} fit finished (took {end_time-start_time:1.3f} [s]).'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1137\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1138\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    413\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m           \u001b[0mnumpy_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\doutorado\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 841.68x595.44 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dump = {}\n",
    "\n",
    "dump['sys'] = {}\n",
    "dump['meta'] = {}\n",
    "dump['exec'] = {}\n",
    "\n",
    "for code in codes:\n",
    "    \n",
    "    dump['exec'][code] = {}\n",
    "    \n",
    "    # Create all possible information words\n",
    "    d = np.zeros((2**k,k),dtype=bool)\n",
    "    for i in range(1,2**k):\n",
    "        d[i]= inc_bool(d[i-1])\n",
    "\n",
    "    # Create sets of all possible codewords (codebook)\n",
    "    if code == 'polar':   \n",
    "\n",
    "        A = polar_design_awgn(N, k, design_snr_dB=0)  # logical vector indicating the nonfrozen bit locations \n",
    "        x = np.zeros((2**k, N),dtype=bool)\n",
    "        u = np.zeros((2**k, N),dtype=bool)\n",
    "        u[:,A] = d\n",
    "\n",
    "        for i in range(0,2**k):\n",
    "            x[i] = polar_transform_iter(u[i])\n",
    "\n",
    "    elif code == 'random':\n",
    "\n",
    "        np.random.seed(4267)   # for a 16bit Random Code (r=0.5) with Hamming distance >= 2\n",
    "        x = np.random.randint(0,2,size=(2**k,N), dtype=bool)\n",
    "\n",
    "    for nb_epoch in nb_epochs:\n",
    "        \n",
    "        nb_epoch_exp = int(np.log2(nb_epoch))\n",
    "        \n",
    "        dump['exec'][code][nb_epoch] = {}\n",
    "        \n",
    "        # https://stackoverflow.com/questions/15571267/python-a4-size-for-a-plot\n",
    "        fig = plt.figure(figsize=(11.69,8.27))\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            \n",
    "            dump['exec'][code][nb_epoch][optimizer] = {}\n",
    "\n",
    "            ###################\n",
    "            # Define NN model #\n",
    "            ###################\n",
    "\n",
    "            # Define modulator\n",
    "            modulator_layers = [Lambda(modulateBPSK, \n",
    "                                      input_shape=(N,), output_shape=return_output_shape, name=\"modulator\")]\n",
    "            modulator = compose_model(modulator_layers)\n",
    "            modulator.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            # Define noise\n",
    "            noise_layers = [Lambda(addNoise, arguments={'sigma':train_sigma}, \n",
    "                                   input_shape=(N,), output_shape=return_output_shape, name=\"noise\")]\n",
    "            noise = compose_model(noise_layers)\n",
    "            noise.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            # Define LLR\n",
    "            llr_layers = [Lambda(log_likelihood_ratio, arguments={'sigma':train_sigma}, \n",
    "                                 input_shape=(N,), output_shape=return_output_shape, name=\"LLR\")]\n",
    "            llr = compose_model(llr_layers)\n",
    "            llr.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            # Define decoder \n",
    "            decoder_layers = [Dense(design[0], activation='relu', input_shape=(N,))]\n",
    "            for i in range(1,len(design)):\n",
    "                decoder_layers.append(Dense(design[i], activation='relu'))\n",
    "            decoder_layers.append(Dense(k, activation='sigmoid'))\n",
    "            decoder = compose_model(decoder_layers)\n",
    "            decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "\n",
    "            # Define model\n",
    "            if LLR:\n",
    "                model_layers = modulator_layers + noise_layers + llr_layers + decoder_layers\n",
    "            else:\n",
    "                model_layers = modulator_layers + noise_layers + decoder_layers\n",
    "            model = compose_model(model_layers)\n",
    "            model.compile(optimizer=optimizer, loss=loss, metrics=[ber])\n",
    "\n",
    "            ############\n",
    "            # Train NN #\n",
    "            ############    \n",
    "\n",
    "            model.summary()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            print(f'{datetime.now()}\\t{code} @ {optimizer} @ Mep=2^{nb_epoch_exp} fit started.')\n",
    "            history = model.fit(x, d, batch_size=batch_size, epochs=nb_epoch, verbose=0, shuffle=True)\n",
    "            end_time = timeit.default_timer()\n",
    "            print(f'{datetime.now()}\\t{code} @ {optimizer} @ Mep=2^{nb_epoch_exp} fit finished (took {end_time-start_time:1.3f} [s]).')\n",
    "\n",
    "            ###########\n",
    "            # Test NN #\n",
    "            ###########\n",
    "\n",
    "            test_batch = 1000  \n",
    "            num_words = 100000      # multiple of test_batch\n",
    "\n",
    "            SNR_dB_start_Eb = 0\n",
    "            SNR_dB_stop_Eb = 10\n",
    "            SNR_points = SNR_dB_stop_Eb - SNR_dB_start_Eb + 1\n",
    "\n",
    "            SNR_dB_start_Es = SNR_dB_start_Eb + 10*np.log10(k/N)\n",
    "            SNR_dB_stop_Es = SNR_dB_stop_Eb + 10*np.log10(k/N)\n",
    "\n",
    "            sigma_start = np.sqrt(1/(2*10**(SNR_dB_start_Es/10)))\n",
    "            sigma_stop = np.sqrt(1/(2*10**(SNR_dB_stop_Es/10)))\n",
    "\n",
    "            sigmas = np.linspace(sigma_start, sigma_stop, SNR_points)\n",
    "            sigmas_db = 10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N)\n",
    "            print(f'{datetime.now()}\\ttest @ sigmas={sigmas}')\n",
    "            print(f'{datetime.now()}\\ttest @ sigmas(dB)={sigmas_db}')\n",
    "\n",
    "            nb_errors = np.zeros(len(sigmas),dtype=int)\n",
    "            nb_bits = np.zeros(len(sigmas),dtype=int)\n",
    "\n",
    "            for i in range(0,len(sigmas)):\n",
    "                \n",
    "                np.random.seed(0)\n",
    "\n",
    "                for ii in range(0,np.round(num_words/test_batch).astype(int)):\n",
    "\n",
    "                    # Source\n",
    "                    d_test = np.random.randint(0,2,size=(test_batch,k)) \n",
    "\n",
    "                    # Encoder\n",
    "                    if code == 'polar':\n",
    "                        x_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                        u_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                        u_test[:,A] = d_test\n",
    "\n",
    "                        for iii in range(0,test_batch):\n",
    "                            x_test[iii] = polar_transform_iter(u_test[iii])\n",
    "\n",
    "                    elif code == 'random':\n",
    "                        x_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                        for iii in range(0,test_batch):\n",
    "                            x_test[iii] = x[bin2int(d_test[iii])]\n",
    "\n",
    "                    # Modulator (BPSK)\n",
    "                    s_test = -2*x_test + 1\n",
    "\n",
    "                    # Channel (AWGN)\n",
    "                    y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "                    if LLR:\n",
    "                        y_test = 2*y_test/(sigmas[i]**2)\n",
    "\n",
    "                    # Decoder\n",
    "                    nb_errors[i] += decoder.evaluate(y_test, d_test, batch_size=test_batch, verbose=0)[1]\n",
    "                    nb_bits[i] += d_test.size\n",
    "\n",
    "                print(f'{datetime.now()}\\ttest @ sigma[{i}]={sigmas[i]:.3f}\\tsigma_db[{i}]={sigmas_db[i]:.3f}\\tnb_bits={nb_bits[i]}\\tnb_errors={nb_errors[i]}')\n",
    "\n",
    "            #######################\n",
    "            # Plot Bit-Error-Rate #\n",
    "            #######################\n",
    "\n",
    "            # NN decoder\n",
    "            x_nn = sigmas_db\n",
    "            y_nn = nb_errors/nb_bits\n",
    "            plt.semilogy(x_nn, y_nn, label=optimizer.capitalize(), linestyle='solid', linewidth=2.0)\n",
    "\n",
    "            ###################################\n",
    "            # Persist current results to JSON #\n",
    "            ###################################\n",
    "\n",
    "            dump['exec'][code][nb_epoch][optimizer]['x_nn'] = x_nn.tolist()\n",
    "            dump['exec'][code][nb_epoch][optimizer]['y_nn'] = y_nn.tolist()\n",
    "        \n",
    "        ############\n",
    "        # Figure 3 #\n",
    "        ############\n",
    "        \n",
    "        # MLE decoder\n",
    "        x_mle = sigmas_db\n",
    "        y_mle = [theoretic_symbol_error_rate(Es_No_dB) for Es_No_dB in sigmas_db]\n",
    "        dump['exec'][code][nb_epoch][optimizer]['x_mle'] = x_mle.tolist()\n",
    "        dump['exec'][code][nb_epoch][optimizer]['y_mle'] = y_mle\n",
    "        plt.semilogy(x_mle, y_mle, label='MLE', linestyle='dotted', linewidth=2.0, color='black')\n",
    "        \n",
    "        plt.grid(True, which=\"both\")\n",
    "        plt.legend()\n",
    "        plt.xlabel('$E_b/N_0$ [dB]')\n",
    "        plt.ylabel('BER')\n",
    "        plt.xlim(SNR_dB_start_Eb, SNR_dB_stop_Eb)\n",
    "        plt.ylim(1e-5, 1e0)\n",
    "        plt.title(code.capitalize() + ' code ($M_{ep}=2^{' + str(nb_epoch_exp) + '})$')\n",
    "        plt.savefig(f'code={code}_epochs=2^{nb_epoch_exp}_mle.png')\n",
    "        plt.show()\n",
    "        \n",
    "        ################################\n",
    "        # Persist current plot to JSON #\n",
    "        ################################\n",
    "        \n",
    "        with open(f'code={code}_epochs=2^{nb_epoch_exp}_mle.png', 'rb') as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        dump['exec'][code][nb_epoch]['plot_base64'] = encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Persist metadata to JSON #\n",
    "############################\n",
    "            \n",
    "dump['sys']['v_info'] = sys.version_info\n",
    "dump['sys']['v_numpy'] = np.__version__\n",
    "dump['sys']['v_keras'] = k_tmp.__version__\n",
    "dump['sys']['v_tensorflow'] = tf_tmp.__version__\n",
    "dump['sys']['is_gpu_available'] = tf.test.is_gpu_available(cuda_only=True)\n",
    "dump['sys']['local_devices'] = str(device_lib.list_local_devices())\n",
    "dump['sys']['keras_backend'] = K.backend()\n",
    "dump['sys']['datetime'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "dump['meta']['k'] = k\n",
    "dump['meta']['N'] = N\n",
    "dump['meta']['train_SNR_Eb'] = train_SNR_Eb\n",
    "dump['meta']['nb_epochs'] = nb_epochs\n",
    "dump['meta']['codes'] = codes\n",
    "dump['meta']['design'] = design\n",
    "dump['meta']['batch_size'] = batch_size\n",
    "dump['meta']['LLR'] = LLR\n",
    "dump['meta']['loss'] = loss\n",
    "dump['meta']['optimizers'] = optimizers\n",
    "dump['meta']['train_SNR_Es'] = train_SNR_Es\n",
    "dump['meta']['train_sigma'] = train_sigma\n",
    "dump['meta']['test_batch'] = test_batch\n",
    "dump['meta']['num_words'] = num_words\n",
    "dump['meta']['SNR_dB_start_Eb'] = SNR_dB_start_Eb\n",
    "dump['meta']['SNR_dB_stop_Eb'] = SNR_dB_stop_Eb\n",
    "dump['meta']['SNR_points'] = SNR_points\n",
    "dump['meta']['sigmas'] = sigmas.tolist()\n",
    "dump['meta']['sigmas_db'] = sigmas_db.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Persist JSON to filesystem #\n",
    "##############################\n",
    "\n",
    "with open(f'code={code}_mle.json', 'w') as outfile: \n",
    "    json.dump(dump, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
